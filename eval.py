# chat.py
def init_model(args):
    print("âœ… åŠ è½½æ¨¡å‹...")
    moe_suffix = '_moe' if args.use_moe else ''
    ckpt_path = os.path.join(args.save_dir, f"{args.weight}_{args.hidden_size}{moe_suffix}.pth")
    config = MiniMindConfig(hidden_size=args.hidden_size, num_hidden_layers=args.num_hidden_layers, use_moe=args.use_moe)
    model = MiniMindForCausalLM(config)
    state_dict = torch.load(ckpt_path, map_location=args.device)
    model.load_state_dict(state_dict)
    print("âœ… åŠ è½½å‚æ•°...")
    model.to(args.device).eval()
    print("âœ… åŠ è½½ç¼–ç å™¨...")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, use_fast=False)
    print("âœ… å…¨éƒ¨åŠ è½½æˆåŠŸï¼å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    return model,tokenizer

def eval(args,prompts):
    model,tokenizer = init_model(args)
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯• [1] æ‰‹åŠ¨è¾“å…¥ : '))
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    
    conversation = []
    for prompt in prompt_iter:
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})
        templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
        # ä½¿ç”¨tokenizer_configä¸­çš„chat_templateæ„é€ è¾“å…¥prompt
        inputs = tokenizer.apply_chat_template(**templates) if args.eval_mode == "sft" else tokenizer.bos_token + prompt
        inputs = tokenizer(inputs, return_tensors="pt").to(args.device)
        if input_mode ==0: print(f'ğŸ’¬: {prompt}')
        st = time.time()
        response = ""
        outputs_id = [[]] # [1,input_len]
        out_text_len = 0
        print(f"prompt:{prompt}")
        with torch.no_grad():
            if args.stream:
                print(f'ğŸ¤–: ',end="")
                for token_id in model.generate_stream(
                    inputs["input_ids"],
                    temperature=0.7,
                    top_p=0.9,
                    eos_token_id=tokenizer.eos_token_id
                ):
                    outputs_id[0].append(token_id)
                    # âœ… å…³é”®ï¼šdecode æ•´ä¸ªåºåˆ—ï¼Œä¸æ˜¯å•ä¸ª token
                    response = tokenizer.decode(outputs_id[0], skip_special_tokens=True)
                    # åªè¾“å‡ºå€’æ•°ç¬¬ä¸‰ä¸ªå­—ç¬¦ï¼Œé¿å…æ–°è¾“å‡ºtokenç”±äºä¸å®Œæ•´å¯¼è‡´ä¹±ç ã€‚
                    print(response[out_text_len:-3], end="", flush=True)
                    out_text_len = out_text_len + len(response[out_text_len:-3])
                print(response[-3:], end="", flush=True)
            else:
                outputs_id = model.generate(
                    input_ids=inputs["input_ids"],
                    # attention_mask=inputs["attention_mask"],
                    temperature=0.7,
                    top_p=0.9,
                    eos_token_id=tokenizer.eos_token_id  # å…³é”®ï¼šç”¨ <|im_end|> ä½œä¸ºç»“æŸç¬¦
                )
                response = tokenizer.decode(outputs_id[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
                print(f'ğŸ¤–: {response}')
        conversation.append({"role": "assistant", "content": response})
        gen_tokens = len(outputs_id[0]) - len(inputs["input_ids"][0])
        print(f'[Speed]: {gen_tokens / (time.time() - st):.2f} tokens/s\n') if args.show_speed else print('\n\n')

if __name__ == "__main__":
    print("âœ… æ­£åœ¨åŠ è½½ä¾èµ–åº“...")
    import argparse
    import os
    import time
    import torch
    from transformers import AutoTokenizer
    from config import MiniMindConfig
    from model_luming import MiniMindForCausalLM  # å‡è®¾ä½ çš„æ¨¡å‹å®šä¹‰åœ¨ model.py ä¸­
    from utils import setup_seed
    print("âœ… æ­£åœ¨è§£æå‚æ•°...")
    setup_seed(42)
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument('--tokenizer_path', default='tokenizer/minimind', type=str, help="tokenizeræ•°æ®åŠ è½½è·¯å¾„")
    parser.add_argument('--save_dir', default='out', type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='sft', type=str, help="æƒé‡åç§°å‰ç¼€")
    parser.add_argument('--hidden_size', default=768, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=16, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--max_new_tokens', default=256, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=3, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--show_speed', default=1, type=int, help="æ˜¾ç¤ºdecodeé€Ÿåº¦ï¼ˆtokens/sï¼‰")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
    parser.add_argument('--eval_mode', default='pretrain', type=str, choices=["pretrain","sft"], help="æµ‹è¯•ç±»å‹[pretrain/sft]")
    parser.add_argument('--stream', default=0, type=int, choices=[0,1], help="æ˜¯å¦æµå¼è¾“å‡º?(æ˜¯/å¦)[1]/[0]")
    args = parser.parse_args()
    print("âœ… å‚æ•°è§£æå®Œæˆ")
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',
        'è¯·å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹ã€‚',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿ',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹ã€‚',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿã€‚'
    ]
    eval(args,prompts)