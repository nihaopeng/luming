print("âœ… æ­£åœ¨åŠ è½½ä¾èµ–åº“...")
import argparse
import time
import torch
import torch.nn.functional as F
from utils import TokenConfig, setup_seed, Logger
from transformers import AutoTokenizer,AutoModelForCausalLM

def eval_args():
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument('--tokenizer_path', default='./tokenizer/qwen0.6Bbase', type=str, help="tokenizeræ•°æ®åŠ è½½è·¯å¾„")
    parser.add_argument('--from_weight', default='./out/qwen_sft', type=str, help="æƒé‡è·¯å¾„ï¼ŒåŒ…å«æ–‡ä»¶å")
    parser.add_argument('--hidden_size', default=768, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=16, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=2, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--show_speed', default=1, type=int, help="æ˜¾ç¤ºdecodeé€Ÿåº¦ï¼ˆtokens/sï¼‰")
    parser.add_argument("--sep", type=str, default="<|im_start|>assistant,<|im_end|>,<|endoftext|>", help="å¾®è°ƒä½¿ç”¨çš„èµ·å§‹tokenï¼Œç»“æŸtokenå’Œå¡«å……token")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
    parser.add_argument('--eval_mode', default='sft', type=str, choices=["pretrain","sft"], help="æµ‹è¯•ç±»å‹[pretrain/sft]")
    parser.add_argument('--stream', default=1, type=int, choices=[0,1], help="æ˜¯å¦æµå¼è¾“å‡º?(æ˜¯/å¦)[1]/[0]")
    parser.add_argument('--max_seq_len', default=340, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦ï¼ˆä¸­æ–‡1tokenâ‰ˆ1.5~1.7å­—ç¬¦ï¼‰")
    args = parser.parse_args()
    return args

def init_model(args):
    Logger("âœ… åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    Logger("âœ… åŠ è½½å‚æ•°...")
    model = AutoModelForCausalLM.from_pretrained(
        args.from_weight,  # æˆ–æœ¬åœ°è·¯å¾„åŒ…å« model.safetensors
    )
    Logger("âœ… å…¨éƒ¨åŠ è½½æˆåŠŸï¼")
    Logger(f'Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M')
    return model.to(args.device), tokenizer

def stream_generate(
    model,
    tokenizer,
    token_config: TokenConfig,
    prompt: str,
    max_new_tokens: int = 128,
    temperature: float = 1.0,
    top_p: float = 1.0,
    stop_on_eos: bool = True,
):
    """
    æµå¼ç”Ÿæˆï¼Œä½†åŸºäºå®Œæ•´ token åºåˆ—è§£ç åæŒ‰å­—ç¬¦é€æ­¥ yieldã€‚
    è¡Œä¸ºï¼š
      - æ¯æ¬¡ç”Ÿæˆæ–° token åï¼Œå°†æ‰€æœ‰ new_token_ids æ•´ä½“ decode ä¸º output_textã€‚
      - å½“ output_text é•¿åº¦ >= 3 æ—¶ï¼Œå¼€å§‹ yield å­—ç¬¦ï¼ˆä»ç¬¬0ä¸ªå¼€å§‹ï¼‰ã€‚
      - ä¹‹åæ¯æ­¥ yield ä¸€ä¸ªæ–°å­—ç¬¦ï¼ˆå³ output_text[len(yielded_chars)]ï¼‰ã€‚
      - æœ€åç¡®ä¿æ‰€æœ‰å­—ç¬¦éƒ½è¢« yieldï¼ˆåŒ…æ‹¬æœ«å°¾å¯èƒ½å›  token è¾¹ç•Œå»¶è¿Ÿå‡ºç°çš„éƒ¨åˆ†ï¼‰ã€‚
    """
    device = model.device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    input_ids = inputs.input_ids
    attention_mask = inputs.attention_mask
    past_key_values = None
    new_token_ids: list[int] = []
    yielded_char_count = 0  # å·²ç» yield çš„å­—ç¬¦æ•°
    with torch.no_grad():
        for step in range(max_new_tokens):
            if step == 0:
                current_input = input_ids
            else:
                current_input = new_token_id.unsqueeze(0)
            outputs = model(
                input_ids=current_input,
                attention_mask=attention_mask,
                past_key_values=past_key_values,
                use_cache=True,
            )
            past_key_values = outputs.past_key_values
            next_token_logits = outputs.logits[:, -1, :]
            # Temperature
            if temperature != 1.0:
                next_token_logits = next_token_logits / temperature
            # Top-p sampling
            if top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
                sorted_indices_to_remove = cumulative_probs > top_p
                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                sorted_indices_to_remove[..., 0] = False
                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                next_token_logits = next_token_logits.masked_fill(indices_to_remove, -float("inf"))
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1).squeeze(1)
            token_id = next_token.item()
            # Check EOS
            if stop_on_eos and token_id == tokenizer.convert_tokens_to_ids(token_config.response_end_token):
                break
            new_token_ids.append(token_id)
            # Decode the full sequence of new tokens
            output_text = tokenizer.decode(new_token_ids, skip_special_tokens=True)
            # Yield characters one by one that haven't been yielded yet
            while yielded_char_count < len(output_text)-3:
                yield output_text[yielded_char_count]
                yielded_char_count += 1
            # Update state for next iteration
            new_token_id = next_token
            attention_mask = torch.cat([attention_mask, attention_mask.new_ones((1, 1))], dim=-1)
        # Final flush: in case decoding after loop adds more characters (e.g., due to BPE merging)
        final_output_text = tokenizer.decode(new_token_ids, skip_special_tokens=True)
        while yielded_char_count < len(final_output_text):
            yield final_output_text[yielded_char_count]
            yielded_char_count += 1

def eval(args,prompts):
    model,tokenizer = init_model(args)
    sep = args.sep.split(",")
    Logger(sep)
    assert len(sep)==3,"sepå‚æ•°éœ€è¦ä¸‰ä¸ªtokenï¼Œç”¨é€—å·åˆ†éš”"
    token_config = TokenConfig(sep[0],sep[1],sep[2])
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯• [1] æ‰‹åŠ¨è¾“å…¥ : '))
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    conversation = []
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    for prompt in prompt_iter:
        if prompt == "quit" or prompt == "exit": break
        setup_seed(2026) # or setup_seed(random.randint(0, 2048))
        if input_mode == 0: print(f'ğŸ’¬: {prompt}')
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})

        templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
        input_full_prompt = tokenizer.apply_chat_template(**templates) if args.eval_mode=="sft" else token_config.response_start_token + prompt
        response = ""
        print(f'ğŸ¤–: ',end="")
        for token_str in stream_generate(model,tokenizer,token_config,input_full_prompt,max_new_tokens=args.max_seq_len):
            print(token_str,end="",flush=True)
            response += token_str
        print()
        conversation.append({"role": "assistant", "content": response})

if __name__ == "__main__":
    
    print("âœ… æ­£åœ¨è§£æå‚æ•°...")
    setup_seed(42)
    args = eval_args()
    print("âœ… å‚æ•°è§£æå®Œæˆ")
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',
        'è¯·å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹ã€‚',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿ',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹ã€‚',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿã€‚'
    ]
    eval(args,prompts)