print("âœ… æ­£åœ¨åŠ è½½ä¾èµ–åº“...")
import argparse
import time
import torch
from utils import TokenConfig, setup_seed, Logger
from transformers import AutoTokenizer,AutoModelForCausalLM

def init_model(args):
    Logger("âœ… åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)
    Logger("âœ… åŠ è½½å‚æ•°...")
    model = AutoModelForCausalLM.from_pretrained(
        args.from_weight,  # æˆ–æœ¬åœ°è·¯å¾„åŒ…å« model.safetensors
    )
    Logger("âœ… å…¨éƒ¨åŠ è½½æˆåŠŸï¼")
    Logger(f'Trainable Params: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.3f}M')
    return model.to(args.device), tokenizer

class miniStreamer:
    def __init__(self,args,tokenizer):
        self.args = args
        self.tokenizer = tokenizer
        self.output_ids = []
        self.output_text_idx = 0
        self.response = ""
        self.is_first_output = True
    
    def put(self, value):
        ids = value.tolist()
        if self.is_first_output:
            print(f'ğŸ¤–: ',end="")
            self.is_first_output = False
            ids = [ids[0][-1]] # ç¬¬ä¸€æ¬¡ä¼šå°†promptä¹Ÿä¼ è¿›æ¥ï¼Œæ­¤æ—¶å–æœ€åä¸€ä¸ªtokenå³å¯ã€‚
        self.output_ids.extend(ids)
        text = self.tokenizer.decode(self.output_ids, skip_special_tokens=True)
        if self.args.stream and self.output_text_idx < len(self.response)-3:
            print(self.response[self.output_text_idx], end='', flush=True)
            self.output_text_idx += 1
        self.response = text
    
    def end(self):
        # æ ¹æ® stream å¼€å…³å†³å®šæ˜¯å¦è¾“å‡º
        if self.args.stream:
            print(self.response[self.output_text_idx+1:])  # æµå¼æ¨¡å¼ï¼šç»“æŸæ—¶æ¢è¡Œ
        else:
            print(f'{self.response}')  # éæµå¼ï¼šä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´ response
        self.output_ids = []
        self.output_text_idx = 0
        self.response = ""
        self.is_first_output = True

def eval(args,prompts):
    model,tokenizer = init_model(args)
    sep = args.sep.split(",")
    print(sep)
    assert len(sep)==3,"sepå‚æ•°éœ€è¦ä¸‰ä¸ªtokenï¼Œç”¨é€—å·åˆ†éš”"
    token_config = TokenConfig(sep[0],sep[1],sep[2])
    input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯• [1] æ‰‹åŠ¨è¾“å…¥ : '))
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    
    conversation = []
    streamer = miniStreamer(args,tokenizer)
    prompt_iter = prompts if input_mode == 0 else iter(lambda: input('ğŸ’¬: '), '')
    for prompt in prompt_iter:
        if prompt == "quit" or prompt == "exit": break
        setup_seed(2026) # or setup_seed(random.randint(0, 2048))
        if input_mode == 0: print(f'ğŸ’¬: {prompt}')
        conversation = conversation[-args.historys:] if args.historys else []
        conversation.append({"role": "user", "content": prompt})

        templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
        inputs = tokenizer.apply_chat_template(**templates) if args.eval_mode=="sft" else token_config.response_start_token + prompt
        inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

        st = time.time()
        generated_ids = model.generate(
            inputs=inputs.input_ids, attention_mask=inputs.attention_mask,
            max_new_tokens=args.max_seq_len, streamer=streamer,
            pad_token_id=tokenizer.convert_tokens_to_ids(token_config.pad_token),
            eos_token_id=tokenizer.convert_tokens_to_ids(token_config.response_end_token),
            top_p=args.top_p, temperature=args.temperature
        )
        conversation.append({"role": "assistant", "content": streamer.response})
        gen_tokens = len(generated_ids[0]) - len(inputs["input_ids"][0])
        print(f'\n[Speed]: {gen_tokens / (time.time() - st):.2f} tokens/s\n\n') if args.show_speed else print('\n\n')

if __name__ == "__main__":
    
    print("âœ… æ­£åœ¨è§£æå‚æ•°...")
    setup_seed(42)
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument('--tokenizer_path', default='tokenizer/minimind', type=str, help="tokenizeræ•°æ®åŠ è½½è·¯å¾„")
    parser.add_argument('--from_weight', default='sft', type=str, help="æƒé‡è·¯å¾„ï¼ŒåŒ…å«æ–‡ä»¶å")
    parser.add_argument('--hidden_size', default=768, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=16, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--max_new_tokens', default=256, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=0, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--show_speed', default=1, type=int, help="æ˜¾ç¤ºdecodeé€Ÿåº¦ï¼ˆtokens/sï¼‰")
    parser.add_argument("--sep", type=str, default="<|im_start|>assistant,<|im_end|>,<|endoftext|>", help="å¾®è°ƒä½¿ç”¨çš„èµ·å§‹tokenï¼Œç»“æŸtokenå’Œå¡«å……token")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
    parser.add_argument('--eval_mode', default='pretrain', type=str, choices=["pretrain","sft"], help="æµ‹è¯•ç±»å‹[pretrain/sft]")
    parser.add_argument('--stream', default=0, type=int, choices=[0,1], help="æ˜¯å¦æµå¼è¾“å‡º?(æ˜¯/å¦)[1]/[0]")
    parser.add_argument('--max_seq_len', default=340, type=int, help="è®­ç»ƒçš„æœ€å¤§æˆªæ–­é•¿åº¦ï¼ˆä¸­æ–‡1tokenâ‰ˆ1.5~1.7å­—ç¬¦ï¼‰")
    args = parser.parse_args()
    print("âœ… å‚æ•°è§£æå®Œæˆ")
    prompts = [
        'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
        'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„ï¼Ÿ',
        'è¯·å†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
        'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹ã€‚',
        'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨ï¼Ÿ',
        'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹ã€‚',
        'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
        'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿã€‚'
    ]
    eval(args,prompts)