# chat.py
import os
import torch
from transformers import AutoTokenizer
from config import MiniMindConfig
from model_luming import MiniMindForCausalLM  # å‡è®¾ä½ çš„æ¨¡å‹å®šä¹‰åœ¨ model.py ä¸­
from utils import setup_seed

def main():
    # === é…ç½® ===
    device = "cuda" if torch.cuda.is_available() else "cpu"
    hidden_size = 768          # æ ¹æ®ä½ è®­ç»ƒæ—¶çš„é…ç½®è°ƒæ•´
    num_hidden_layers = 16     # åŒä¸Š
    use_moe = False            # æ ¹æ®å®é™…è®¾ç½®
    save_dir = "./out"
    weight_name = "pretrain"   # ä¸è®­ç»ƒæ—¶ args.save_weight ä¸€è‡´
    moe_suffix = '_moe' if use_moe else ''
    ckpt_path = os.path.join(save_dir, f"{weight_name}_{hidden_size}{moe_suffix}.pth")

    # === åˆå§‹åŒ–æ¨¡å‹å’Œ tokenizer ===
    config = MiniMindConfig(hidden_size=hidden_size, num_hidden_layers=num_hidden_layers, use_moe=use_moe)
    model = MiniMindForCausalLM(config)
    
    # åŠ è½½æƒé‡ï¼ˆæ³¨æ„ï¼šè®­ç»ƒæ—¶ä¿å­˜çš„æ˜¯ .half()ï¼Œæ‰€ä»¥éœ€ map_location + float16ï¼‰
    state_dict = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(state_dict)
    model.to(device).eval()

    # tokenizerï¼ˆå‡è®¾ä½ ç”¨çš„æ˜¯ç±»ä¼¼ LLaMA çš„ tokenizerï¼Œæˆ–è‡ªå®šä¹‰çš„ï¼‰
    # å¦‚æœä½ æ²¡æœ‰ç”¨ transformers tokenizerï¼Œè€Œæ˜¯è‡ªå®šä¹‰çš„ï¼Œè¯·æ›¿æ¢ä¸ºä½ çš„ tokenizer
    tokenizer = AutoTokenizer.from_pretrained("./tokenizer", use_fast=False)  # ä»…ç”¨äº tokenizationï¼Œä¸åŠ è½½æ¨¡å‹

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰\n")

    # === å¤šè½®å¯¹è¯å†å² ===
    history = []

    while True:
        user_input = input("ğŸ‘¤ ç”¨æˆ·: ").strip()
        if user_input.lower() in ["quit", "exit"]:
            break

        # æ„é€ å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆæŒ‰ä½ çš„è®­ç»ƒæ ¼å¼ï¼‰
        # è®­ç»ƒæ•°æ®æ ¼å¼ï¼š<|im_start|>...<|im_end|>
        history.append(f"<|im_start|>{user_input}")
        prompt = " ".join(history)

        # ç¼–ç 
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        input_ids = inputs.input_ids
        attention_mask = inputs.attention_mask

        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                input_ids=input_ids,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id  # å…³é”®ï¼šç”¨ <|im_end|> ä½œä¸ºç»“æŸç¬¦
            )

        # è§£ç ç”Ÿæˆéƒ¨åˆ†
        generated_ids = outputs[0][input_ids.shape[1]:]
        response = tokenizer.decode(generated_ids, skip_special_tokens=False).strip()

        # ç§»é™¤å¯èƒ½çš„ <|im_end|> åŠä¹‹åå†…å®¹
        if tokenizer.eos_token in response:
            response = response.split(tokenizer.eos_token)[0].strip()

        print(f"ğŸ¤– åŠ©æ‰‹: {response}")

        # å°†åŠ©æ‰‹å›å¤åŠ å…¥å†å²ï¼ˆç”¨äºä¸‹ä¸€è½®ï¼‰
        history.append(f"{response}{tokenizer.eos_token}")

        # å¯é€‰ï¼šé™åˆ¶å†å²é•¿åº¦é˜²æ­¢è¿‡é•¿
        if len(history) > 6:  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            history = history[-4:]

    print("ğŸ‘‹ å†è§ï¼")

if __name__ == "__main__":
    setup_seed(42)
    main()