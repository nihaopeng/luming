# luming - è½»é‡çº§è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶

luming æ˜¯ä¸€ä¸ªç®€æ´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æ¡†æ¶, ä¸»è¦å‚è€ƒ[minimind](https://github.com/jingyaogong/minimind)é¡¹ç›®ï¼ˆå¤§éƒ¨åˆ†ä»£ç æ˜¯ç›´æ¥è¿ç§»çš„ï¼‰ï¼Œä¸“ä¸ºå¿«é€Ÿå…¥é—¨å’Œå®è·µè¯­è¨€æ¨¡å‹è€Œè®¾è®¡ã€‚æ”¯æŒé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒ(SFT)ã€è¯„ä¼°å’Œéƒ¨ç½²çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚æ”¯æŒqwen0.6Bå¾®è°ƒä¸è¯„æµ‹ã€‚

## ç¯å¢ƒ

train
---

+ 1ï¼Œtorch

+ 2ï¼Œtransformer

web
---
`pip install starlette toml uvicorn`

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é¢„è®­ç»ƒ

ä»å¤´å¼€å§‹é¢„è®­ç»ƒ minimind-104M æ¨¡å‹
```bash
CUDA_VISIBLE_DEVICES=1 python train.py --dtype=float16 --data_path "./dataset/pretrain_hq.jsonl" --save_weight "./out/minimind_pretrain" --from_weight "none" --hidden_size 768 --num_hidden_layers 16 --use_compile 1 --epochs 6 --sep "<|im_start|>,<|im_end|>,<|endoftext|>"
```

### 2. æŒ‡ä»¤å¾®è°ƒ (SFT)

åŸºäºminimind-104Mé¢„è®­ç»ƒæ¨¡å‹å¾®è°ƒï¼Œä½¿ç”¨[sft_mini_512](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)æ•°æ®é›†
```bash
CUDA_VISIBLE_DEVICES=1 python train.py --dtype=float16 --data_path "./dataset/sft_mini_512.jsonl" --tokenizer_path "./tokenizer/minimind" --train_mode "sft" --save_weight "./out/minimind_sft" --from_weight "./out/minimind_pretrain" --hidden_size 768 --num_hidden_layers 16 --use_compile 0 --epochs 2 --sep "<|im_start|>assistant,<|im_end|>,<|endoftext|>"
```

åŸºäºqwen0.6Bæ¨¡å‹å¾®è°ƒï¼Œä½¿ç”¨[sft_mini_512](https://www.modelscope.cn/datasets/gongjy/minimind_dataset/files)æ•°æ®é›†
```bash
# ç¼©å°å¾®è°ƒæ•°æ®é›†çš„è§„æ¨¡ï¼Œç”±äºqwen0.6Bå·²ç»å…·æœ‰å¾ˆå¥½çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ‰€ä»¥è¿™é‡Œä¸»è¦åšå¯¹é½å·¥ä½œï¼Œä¸éœ€è¦å¤ªå¤§çš„æ•°æ®é›†ã€‚è®©æ¨¡å‹å­¦åˆ°å›ç­”é—®é¢˜çš„èƒ½åŠ›å³å¯ã€‚
head -n 50000 dataset/sft_mini_512.jsonl  > dataset/sft_mini_512_head_50000.jsonl

CUDA_VISIBLE_DEVICES=1 python train.py --dtype=float16 --data_path "./dataset/sft_mini_512_head_50000.jsonl" --tokenizer_path "./tokenizer/qwen0.6Bbase" --train_mode "sft" --save_weight "./out/qwen_sft" --from_weight "./out/qwen0.6Bbase" --use_compile 0 --epochs 2 --sep "<|im_start|>assistant,<|im_end|>,<|endoftext|>"
```

### 3. æ¨ç†æµ‹è¯•
```bash
# pretrain minimind eval
CUDA_VISIBLE_DEVICES=2 python eval.py --tokenizer_path "./tokenizer/minimind" --from_weight "./out/minimind_pretrain" --sep "<|im_start|>,<|im_end|>,<|endoftext|>" --eval_mode "pretrain" --stream 1

# pretrain qwen0.6B eval
CUDA_VISIBLE_DEVICES=2 python eval.py --tokenizer_path "./tokenizer/qwen0.6Bbase" --from_weight "./out/qwen0.6Bbase" --sep "<|im_start|>,<|im_end|>,<|endoftext|>" --eval_mode "pretrain" --stream 0

# sft minimind eval
CUDA_VISIBLE_DEVICES=2 python eval.py --tokenizer_path "./tokenizer/qwen0.6Bbase" --from_weight "./out/qwen_sft" --sep "<|im_start|>assistant,<|im_end|>,<|endoftext|>" --eval_mode "sft" --stream 1

# sft qwen0.6B eval
CUDA_VISIBLE_DEVICES=2 python eval.py --tokenizer_path "./tokenizer/qwen0.6Bbase" --from_weight "./out/qwen_sft" --sep "<|im_start|>assistant,<|im_end|>,<|endoftext|>" --eval_mode "sft" --stream 1
```

## ğŸš€ web

`pip install starlette toml uvicorn`

`python -m web.main`

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### æ¨¡å‹ç»„ä»¶
- **MiniMindForCausalLM**: ä¸»è¦æ¨¡å‹ç±»ï¼ŒåŒ…å«è¯­è¨€å»ºæ¨¡å¤´ luming:365-373 
- **Attention**: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒ RoPE ä½ç½®ç¼–ç  luming:82-145 
- **FeedForward/MOEFeedForward**: å‰é¦ˆç½‘ç»œï¼Œæ”¯æŒ MoE æ¶æ„ luming:147-278 

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ model_luming.py    # æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval.py           # æ¨ç†è¯„ä¼°è„šæœ¬
â”œâ”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ config.py         # é…ç½®ç±»
â”œâ”€â”€ dataloader.py     # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ run.sh           # å¿«é€Ÿå¯åŠ¨è„šæœ¬
â””â”€â”€ tokenizer/       # åˆ†è¯å™¨æ–‡ä»¶
```

## âš™ï¸ ä¸»è¦ç‰¹æ€§

### 1. çµæ´»çš„æ¨¡å‹é…ç½®
é€šè¿‡ `MiniMindConfig` ç±»é…ç½®æ¨¡å‹å‚æ•° luming:15-48 ï¼š
- æ”¯æŒæ ‡å‡† Transformer å’Œ MoE æ¶æ„
- å¯é…ç½®æ³¨æ„åŠ›å¤´æ•°ã€å±‚æ•°ã€éšè—å±‚ç»´åº¦
- æ”¯æŒ Flash Attention å’Œ RoPE ä½ç½®ç¼–ç 

### 2. å®Œæ•´çš„è®­ç»ƒæµç¨‹
- **é¢„è®­ç»ƒæ¨¡å¼**: ä½¿ç”¨ `PretrainDataset` å¤„ç†åŸå§‹æ–‡æœ¬ luming:99-100 
- **SFTæ¨¡å¼**: ä½¿ç”¨ `SFTDataset` å¤„ç†å¯¹è¯æ•°æ®ï¼Œæ”¯æŒèŠå¤©æ¨¡æ¿
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯å¤§æ‰¹é‡è®­ç»ƒ luming:39-50 

### 3. é«˜æ•ˆæ¨ç†
- **KVç¼“å­˜**: åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ luming:386-398 
- **æµå¼ç”Ÿæˆ**: å®æ—¶è¾“å‡ºç”Ÿæˆå†…å®¹ luming:425-478 
- **é‡‡æ ·ç­–ç•¥**: æ”¯æŒæ¸©åº¦è°ƒèŠ‚å’Œ nucleus sampling luming:400-413 

### æ¨¡å‹æ¨ç†
```python
from model_luming import MiniMindForCausalLM
from config import MiniMindConfig
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config)
tokenizer = AutoTokenizer.from_pretrained('./tokenizer')

# ç”Ÿæˆæ–‡æœ¬
inputs = tokenizer("ä½ å¥½ï¼Œ", return_tensors="pt")
outputs = model.generate(inputs["input_ids"], temperature=0.7, top_p=0.9)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ•°æ®æ ¼å¼**: é¢„è®­ç»ƒæ•°æ®ä½¿ç”¨ JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡æœ¬æ ·æœ¬
2. **æ˜¾å­˜éœ€æ±‚**: Small-26M æ¨¡å‹çº¦éœ€ 2GB æ˜¾å­˜ï¼ŒBase-104M çº¦ 4GB
3. **åˆ†è¯å™¨**: ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œä½äº `tokenizer/` ç›®å½•
4. **MoEè®­ç»ƒ**: ä½¿ç”¨ MoE æ—¶éœ€è¦è°ƒæ•´ `--num_experts_per_tok` ç­‰å‚æ•°

## Notes

è¿™ä¸ª README ä¸“æ³¨äº MiniMind æ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½å’Œå¿«é€Ÿä½¿ç”¨æ–¹æ³•ã€‚æ¡†æ¶è¿˜åŒ…å«è®¸å¤šé«˜çº§ç‰¹æ€§ï¼Œå¦‚ï¼š
- çµæ´»çš„å­¦ä¹ ç‡è°ƒåº¦ luming:147-148 
- å®Œæ•´çš„å‚æ•°ç»Ÿè®¡å’Œåˆ†æå·¥å…· luming:121-131 

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒå„æºç æ–‡ä»¶çš„æ³¨é‡Šå’Œæ–‡æ¡£ã€‚