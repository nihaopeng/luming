# luming - è½»é‡çº§è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶

luming æ˜¯ä¸€ä¸ªç®€æ´é«˜æ•ˆçš„è¯­è¨€æ¨¡å‹è®­ç»ƒå’Œæ¨ç†æ¡†æ¶ï¼Œä¸“ä¸ºå¿«é€Ÿå…¥é—¨å’Œå®è·µè¯­è¨€æ¨¡å‹è€Œè®¾è®¡ã€‚æ”¯æŒé¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒ(SFT)ã€è¯„ä¼°å’Œéƒ¨ç½²çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. é¢„è®­ç»ƒ
```bash
# ä»å¤´å¼€å§‹é¢„è®­ç»ƒ Small-26M æ¨¡å‹
./run.sh pretrain

# é¢„è®­ç»ƒ Base-104M æ¨¡å‹
python train.py --hidden_size 768 --num_hidden_layers 16 --save_weight pretrain
```

### 2. æŒ‡ä»¤å¾®è°ƒ (SFT)
```bash
# åŸºäºé¢„è®­ç»ƒæƒé‡è¿›è¡ŒæŒ‡ä»¤å¾®è°ƒ
./run.sh sft

# æ‰‹åŠ¨æŒ‡å®šå‚æ•°å¾®è°ƒ
python train.py --from_weight pretrain --sft 1 --hidden_size 512 --num_hidden_layers 8
```

### 3. æ¨ç†æµ‹è¯•
```bash
# äº¤äº’å¼å¯¹è¯
./run.sh eval

# æµå¼è¾“å‡º
python eval.py --weight sft --stream 1 --eval_mode sft
```

## ğŸ“Š æ¨¡å‹è§„æ ¼

| æ¨¡å‹ | hidden_size | num_hidden_layers | å‚æ•°é‡ | é€‚ç”¨åœºæ™¯ |
|------|-------------|-------------------|--------|----------|
| Small-26M | 512 | 8 | ~26M | å¿«é€Ÿå®éªŒã€å­¦ä¹  |
| Base-104M | 768 | 16 | ~104M | åŸºç¡€åº”ç”¨ |
| MoE-145M | 640 | 8 | ~145M | é«˜æ•ˆæ¨ç† | luming:24-28 

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### æ¨¡å‹ç»„ä»¶
- **MiniMindForCausalLM**: ä¸»è¦æ¨¡å‹ç±»ï¼ŒåŒ…å«è¯­è¨€å»ºæ¨¡å¤´ luming:365-373 
- **Attention**: å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¯æŒ RoPE ä½ç½®ç¼–ç  luming:82-145 
- **FeedForward/MOEFeedForward**: å‰é¦ˆç½‘ç»œï¼Œæ”¯æŒ MoE æ¶æ„ luming:147-278 

### è®­ç»ƒç³»ç»Ÿ
- **åˆ†å¸ƒå¼è®­ç»ƒ**: æ”¯æŒ DDP å¤š GPU è®­ç»ƒ luming:115-117 
- **æ··åˆç²¾åº¦**: è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ŒèŠ‚çœæ˜¾å­˜ luming:89-92 
- **æ£€æŸ¥ç‚¹ç®¡ç†**: è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤è®­ç»ƒçŠ¶æ€ luming:66-119 

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ model_luming.py    # æ¨¡å‹æ¶æ„å®šä¹‰
â”œâ”€â”€ train.py          # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ eval.py           # æ¨ç†è¯„ä¼°è„šæœ¬
â”œâ”€â”€ utils.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ config.py         # é…ç½®ç±»
â”œâ”€â”€ dataloader.py     # æ•°æ®åŠ è½½å™¨
â”œâ”€â”€ run.sh           # å¿«é€Ÿå¯åŠ¨è„šæœ¬
â””â”€â”€ tokenizer/       # åˆ†è¯å™¨æ–‡ä»¶
```

## âš™ï¸ ä¸»è¦ç‰¹æ€§

### 1. çµæ´»çš„æ¨¡å‹é…ç½®
é€šè¿‡ `MiniMindConfig` ç±»é…ç½®æ¨¡å‹å‚æ•° luming:15-48 ï¼š
- æ”¯æŒæ ‡å‡† Transformer å’Œ MoE æ¶æ„
- å¯é…ç½®æ³¨æ„åŠ›å¤´æ•°ã€å±‚æ•°ã€éšè—å±‚ç»´åº¦
- æ”¯æŒ Flash Attention å’Œ RoPE ä½ç½®ç¼–ç 

### 2. å®Œæ•´çš„è®­ç»ƒæµç¨‹
- **é¢„è®­ç»ƒæ¨¡å¼**: ä½¿ç”¨ `PretrainDataset` å¤„ç†åŸå§‹æ–‡æœ¬ luming:99-100 
- **SFTæ¨¡å¼**: ä½¿ç”¨ `SFTDataset` å¤„ç†å¯¹è¯æ•°æ®ï¼Œæ”¯æŒèŠå¤©æ¨¡æ¿
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯å¤§æ‰¹é‡è®­ç»ƒ luming:39-50 

### 3. é«˜æ•ˆæ¨ç†
- **KVç¼“å­˜**: åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ luming:386-398 
- **æµå¼ç”Ÿæˆ**: å®æ—¶è¾“å‡ºç”Ÿæˆå†…å®¹ luming:425-478 
- **é‡‡æ ·ç­–ç•¥**: æ”¯æŒæ¸©åº¦è°ƒèŠ‚å’Œ nucleus sampling luming:400-413 

## ğŸ› ï¸ ä½¿ç”¨ç¤ºä¾‹

### è‡ªå®šä¹‰è®­ç»ƒ
```bash
python train.py \
    --hidden_size 512 \
    --num_hidden_layers 8 \
    --batch_size 32 \
    --learning_rate 5e-4 \
    --epochs 6 \
    --data_path ./dataset/pretrain_hq.jsonl \
    --save_weight my_model
```

### æ¨¡å‹æ¨ç†
```python
from model_luming import MiniMindForCausalLM
from config import MiniMindConfig
from transformers import AutoTokenizer

# åŠ è½½æ¨¡å‹
config = MiniMindConfig(hidden_size=512, num_hidden_layers=8)
model = MiniMindForCausalLM(config)
tokenizer = AutoTokenizer.from_pretrained('./tokenizer')

# ç”Ÿæˆæ–‡æœ¬
inputs = tokenizer("ä½ å¥½ï¼Œ", return_tensors="pt")
outputs = model.generate(inputs["input_ids"], temperature=0.7, top_p=0.9)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **æ•°æ®æ ¼å¼**: é¢„è®­ç»ƒæ•°æ®ä½¿ç”¨ JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªæ–‡æœ¬æ ·æœ¬
2. **æ˜¾å­˜éœ€æ±‚**: Small-26M æ¨¡å‹çº¦éœ€ 2GB æ˜¾å­˜ï¼ŒBase-104M çº¦ 4GB
3. **åˆ†è¯å™¨**: ä½¿ç”¨è‡ªå®šä¹‰åˆ†è¯å™¨ï¼Œä½äº `tokenizer/` ç›®å½•
4. **MoEè®­ç»ƒ**: ä½¿ç”¨ MoE æ—¶éœ€è¦è°ƒæ•´ `--num_experts_per_tok` ç­‰å‚æ•°

## Notes

è¿™ä¸ª README ä¸“æ³¨äº MiniMind æ¡†æ¶çš„æ ¸å¿ƒåŠŸèƒ½å’Œå¿«é€Ÿä½¿ç”¨æ–¹æ³•ã€‚æ¡†æ¶è¿˜åŒ…å«è®¸å¤šé«˜çº§ç‰¹æ€§ï¼Œå¦‚ï¼š
- è¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œ WandB é›†æˆ
- çµæ´»çš„å­¦ä¹ ç‡è°ƒåº¦ luming:147-148 
- å®Œæ•´çš„å‚æ•°ç»Ÿè®¡å’Œåˆ†æå·¥å…· luming:121-131 

æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚è€ƒå„æºç æ–‡ä»¶çš„æ³¨é‡Šå’Œæ–‡æ¡£ã€‚